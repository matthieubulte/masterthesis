\subsection{Charlier differential Series}


\begin{definition}[Characteristic function]
    Let $X$ be a random vector in $\Rp$. The characteristic function of $X$ is the function $\zeta : \Rp \rightarrow \C$ given by
    \begin{equation*}
        \zeta(t) = \expec{}{\expf{it^\top X}}.
    \end{equation*}
\end{definition}

If $X$ has a density function, the characteristic function of $X$ is equal to the Fourier transform of its density. To facilitate future discussions, we introduce a special notation for the Fourier transform operator $\F$ that will applied both to distribution functions, then denoting the characteristic function, as
\begin{equation} \label{eq-fourier-distrib}
    \F[P](t) = \expec{}{\expf{it^\top X}},
\end{equation}
where $X \sim P$, but also applied to any function $f \in L^1(\Rp)$ with
\begin{equation} \label{eq-fourier-density}
    \F[f](t) = \int_\Rp \expf{it^\top x}f(x)\d x.
\end{equation}
The following theorem shows that under integrability condition, the Fourier transform can be inverted and the characteristic function be used to express the density of a distribution.

\begin{theorem} \label{thm-char-inversion}
    Let $X \sim P$ be a random vector in $\Rp$ with characteristic function $\zeta \in L^1(\Rp)$. Then, the density of $X$ exists and is given by
    \begin{equation} \label{eq-density-via-charfun}
        f_X(x) = \left(2\pi\right)^{-p} \intRp{t}{ \expf{-it^\top x}\zeta(t) }.
    \end{equation}
\end{theorem}

\begin{proof}
    Let $A \subset \Rp$ be a bounded rectangle $A = [a_1, b_1] \times \ldots \times [a_p, b_p]$ with $P(X \in \partial A) = 0$. By Theorem 3.10.4 in \cite{durrett_2019}, we have that
    \begin{equation*}
        P(X \in A) = \lim_{T \rightarrow \infty} \left(2\pi\right)^{-p}\int_{\left[-T, T\right]^p} \zeta(t) \prod_{k=1}^p \frac{\expf{-it_k a_k} - \expf{-it_k b_k}}{i t_k} \d t.
    \end{equation*}
    By rewriting various terms under the integral, one obtains
    \begin{align*}
        P(X \in A) 
        &= \lim_{T \rightarrow \infty} \left(2\pi\right)^{-p}\int_{\left[-T, T\right]^p} \zeta(t) \prod_{k=1}^p \frac{\expf{-it_k a_k} - \expf{-it_k b_k}}{i t_k} \d t \\
        &= \lim_{T \rightarrow \infty} \left(2\pi\right)^{-p}\int_{\left[-T, T\right]^p} \zeta(t) \prod_{k=1}^p \int_{a_k}^{b_k}\expf{-i t_k x_k} \d x_k \d t \\
        &= \lim_{T \rightarrow \infty} \left(2\pi\right)^{-p}\int_{\left[-T, T\right]^p} \zeta(t) \int_A \expf{-i t^\top x} \d x \d t.
    \end{align*}
    Since $\zeta \in L^1(\Rp)$ and since $A$ is bounded, the integrand in the previous equation is integrable, and the limit $T \rightarrow \infty$ can be replaced by the proper integral over $\Rp$. Furthermore, using the same absolute convergence property and by Fubini's Theorem the order of integration can be changed, yielding
    \begin{align*}
        P(X \in A) 
        &= \left(2\pi\right)^{-p}\intRp{t}{\int_A \zeta(t) \expf{-i t^\top x} \d x} \\
        &= \int_A \left(2\pi\right)^{-p} \intRp{t}{\zeta(t) \expf{-i t^\top x} } \d x.
    \end{align*}
    By definition, this shows that the density of $X$ exists and is given by (\ref{eq-density-via-charfun}).
\end{proof}

While being a useful result on its own, Theorem \ref{thm-char-inversion} can also be used to provide the necessary conditions to invert the Fourier transform of a function that is not a density.

\begin{corollary} \label{corr-fourier-inv}
    Suppose that $f \in L^1(\Rp)$ and $\zeta \in L^1(\Rp)$ are related by
    \begin{equation}\label{eq-fourier-trans}
        \zeta(t) = \F[f](x).
    \end{equation}
    Then, it holds that
    \begin{equation} \label{eq-fourier-inv}
        f(x) = \left(2\pi\right)^{-p}\int_\Rp \expf{-i t^\top x} \zeta(t) \d t.
    \end{equation}
\end{corollary}

\begin{proof}
    We decompose $f$ in it's positive and negative part by $f(x) = f^+(x) - f^-(x)$ where $f^+(x) = f(x) \mathbb{1}_{f(x) \geq 0}$ and $f^-(x) = -f(x) \mathbb{1}_{f(x) < 0}$. Then, if $c^+ = \int_\Rp f^+(x) \d x$ and $c^- = \int_\Rp f^-(x) \d x$, the functions $f^+ / c^+$ and $f^- / c^-$ are both densities over $\Rp$ with characteristic functions $\zeta^+$ and $\zeta^-$. We can then replace these quantities in (\ref{eq-fourier-trans}) to have
    \begin{align*}
        \zeta(t) 
        &= \int_\Rp \expf{it^\top x}f(x)\d x \\
        &= c^+ \int_\Rp \expf{it^\top x} \frac{1}{c^+}f^+(x)\d x - c^- \int_\Rp \expf{it^\top x} \frac{1}{c^-}f^-(x)\d x\\
        &= c^+ \zeta^+(t) - c^-\zeta^-(t).
    \end{align*}
    By applying Theorem \ref{thm-char-inversion} to the positive and negative parts of $f$, we obtain that
    \begin{equation*}
        \frac{1}{c^\pm} f^\pm(x) = \left(2\pi\right)^{-p}\int_\Rp \expf{-i t^\top x} \zeta^\pm(t) \d t,
    \end{equation*}
    and hence
    \begin{align*}
        f(x) 
        &= f^+(x) - f^-(x) \\
        &= c^+ \left(2\pi\right)^{-p}\int_\Rp \expf{-i t^\top x} \zeta^+(t) \d t
         - c^- \left(2\pi\right)^{-p}\int_\Rp \expf{-i t^\top x} \zeta^-(t) \d t \\
        &= \left(2\pi\right)^{-p}\int_\Rp \expf{-i t^\top x} \left[ c^+\zeta^+(t) - c^-\zeta^-(t)\right] \d t\\
        &= \left(2\pi\right)^{-p}\int_\Rp \expf{-i t^\top x} \zeta(t) \d t.
    \end{align*}
\end{proof}

This theorem lets us extend the notation introduced in Equation (\ref{eq-fourier-density}) and define the inverse Fourier transform operator $\Finv$ as in Equation (\ref{eq-fourier-inv}),
\begin{equation*}
    \Finv[\zeta](x) = \left(2\pi\right)^{-p}\int_\Rp \expf{-i t^\top x} \zeta(t) \d t.
\end{equation*}

\begin{lemma} \label{lemma-fourier-derivative}
    Let $r \in \N$ and $f : \R \rightarrow \Rp$ such that all partial derivatives of $f$ of order up to $r$ exist and for any $\tilde{s} \in S(r-1)$
    \begin{equation} \label{eq-tails-to-zero}
        \lim_{\norm{x} \rightarrow \infty} \expf{it^\top x}D^{\tilde{s}}f(x) = 0.
    \end{equation}
    Then for any $s \in S(r)$, it holds that
    \begin{equation*}
        \F\left[D^s f \right](t) = (-i)^r t^s \F[f].
    \end{equation*}
\end{lemma}
\begin{proof}
    Let $\tilde{s} = (s_1, \ldots, s_{r-1})$, then, by direct computation of the Fourier transform,
    \begin{align*}
        \F\left[D^s f \right](t) 
        &= (2\pi)^{-p}\intRp{x}{ \expf{it^\top x} D^s f(x) } \\
        &= (2\pi)^{-p}\int_{\R^{-1}} \int_\R 
            \expf{it^\top x} \ddx{x_{s_r}} D^{\tilde{s}} f(x) 
        \d x_{s_r} \d x_{\tilde{s}}.
    \end{align*}
    Integrating by part over the axis $x_{s_r}$ and using Assumption (\ref{eq-tails-to-zero}) gives
    \begin{align*}
        \F\left[D^s f \right](t)  
        &= - (2\pi)^{-p}\intRp{x}{
            (it_{s_r})\expf{it^\top x} D^{\tilde{s}} f(x)
        } \\
        &= -it_{s_r} (2\pi)^{-p}\intRp{x}{
            \expf{it^\top x} D^{\tilde{s}} f(x)
        } \\
        &= -it_{s_r} \F\left[D^{\tilde{s}} f \right](t)
    \end{align*}
    Iterating the previous steps completes the proof.
\end{proof}


\begin{definition}
    The cumulant generating function $K_X : \Rp \rightarrow \R$ of a random vector $X \in \Rp$ is defined as 
    \begin{equation*}
        K_X(t) = \log \expec{X}{\expf{t^\top X}}.
    \end{equation*}
    Let $s \in S_p(k)$ be an index vector of length $k$, then if the involved derivatives exist, we define the $s$-cumulant of $X$ as
    \begin{equation*}
        \kappa_s(X) = D^s K_X(0).
    \end{equation*}
    In contexts where the random variable studied $X$ is clear, we will use the notation $K(t)$ and $\kappa_s$ instead of $K_X(t)$ and $\kappa_s(X)$.
\end{definition}

\begin{example} \label{ex-cumulants-mvn}
    Let $X \sim N(\mu, \Sigma)$ with $\mu \in \Rp$ and $\Sigma \in \Sp$, then the cumulant generating function $K(t; \mu, \Sigma)$ of $X$ is a quadratic function given by
    \begin{equation*}
        K(t; \mu, \Sigma) = t^\top\mu + t^\top\Sigma t.
    \end{equation*}
    It is then clear that all cumulants of $X$ exist, where first order cumulants are the components $\mu$, second order cumulants are the components of $\Sigma$, and cumulants of higher order are 0.
\end{example}

\begin{lemma} \label{lem-cumulants-props}
    Let $X_1, \ldots, X_n \simiid P$, then the following holds for any $s \in S(k)$
    \begin{itemize}
        \item {
        $\kappa_s(X_1 + \ldots + X_n) = n\kappa_s(X_1)$
        }
        \item {
            For all $c \in \R$, $\kappa_s(c X_1) = c^k\kappa_s(X_1)$
        }
        \item {
            For all $c \in \Rp$, $\kappa_s(X_1 + c) =
            \begin{cases}
                \kappa_s(X_1) + c_i, &\text{if}\ s=(i)\\
                \kappa_s(X_1),& \text{otherwise}
            \end{cases}$
        }
    \end{itemize}
\end{lemma}

\begin{remark}
    One can see that the cumulant generating function is closely related to the characteristic function since
    \begin{equation*}
        K(t) 
        = \log \expec{}{\expf{t^\top X}} 
        = \log \expec{}{\expf{i (-i)t^\top X}}
        = \logf{\zeta(-it)}.
    \end{equation*}
    This equality also allows us to define the cumulants $\kappa_s$ for $s \in S_p(k)$ in terms of the characteristic function
    \begin{equation*}
        \kappa_s = D^s K(0) 
        = \frac{\d^k}{\d x_{s_1} \ldots \d x_{s_k}} \log \zeta(-it) \bigg|_{t=0}
        = (-i)^{k} D^s \log \zeta(0),
    \end{equation*}
    and hence
    \begin{equation*}
        D^s \log \zeta(0) = i^k \kappa_s.
    \end{equation*}
\end{remark}
We now present a heuristic development of the idea behind the Edgeworth expansion. Consider two distributions $P$ and $Q$ over $\Rp$ with densities $f$ and $q$, characteristic functions $\zeta$ and $\xi$, and cumulants $\kappa_s$ and $\gamma_s$ for $s \in S(k)$, $k \in \N$. Assume that both $P$ and $Q$ have a mean equal to $0$ and a covariance matrix equal to $\mathbb{1}_p$. We wish to utilize the cumulants of both distribution to construct an approximation of $P$.

By formal expansion of the difference between the cumulant generating functions of $P$ and $Q$ around 0, we obtain for any $t \in \Rp$
\begin{align*}
    \log \frac{\zeta(t)}{\xi(t)}
    = \log \zeta(t) - \log \xi(t) 
    &= \sum_{r=0}^\infty \sum_{s \in S(r)} (\kappa_s - \gamma_s)\frac{i^r t^s}{r!}\\
    &= \sum_{r=3}^\infty \sum_{s \in S(r)} (\kappa_s - \gamma_s)\frac{i^r t^s}{r!},
\end{align*}
where the last equality holds from the assumption of shared mean and covariance of $P$ and $Q$. Exponentiating on both sides of the equation and isolating $\zeta(t)$, we find that
\begin{equation*}
    \zeta(t) = \xi(t)\expfc{\sum_{r=3}^\infty \sum_{s \in S(r)} (\kappa_s - \gamma_s)\frac{i^r t^s}{r!}}.
\end{equation*}
\
Let $\alpha_s = \kappa_s - \gamma_s$, we can then continue by taking a formal expansion of the exponential function to find
\begin{align*}
    \zeta(t)
    &= \xi(t)\expfc{\sum_{r=3}^\infty \sum_{s \in S(r)} \alpha_s\frac{i^r t^s}{r!}}\\
    &= \xi(t)\sum_{j=0}^\infty \frac{1}{j!} \left\{\sum_{r=3}^\infty \sum_{s \in S(r)} \alpha_s\frac{i^r t^s}{r!}\right\}^j \\
    &=
    \sum_{j=0}^\infty \frac{1}{j!} 
    \sum_{\substack{r_1 = 3\\ \ldots \\r_j = 3}}^\infty
    \sum_{\substack{s_1 \in S(r_1)\\ \ldots \\s_j \in S(r_j)}}
    \alpha_{s_1}\ldots\alpha_{s_j}
    \frac{
        \xi(t) i^{r_1 + \ldots + r_j}
        t^{s_1} \ldots t^{s_j}
    }{
        r_1! \ldots r_j!
    }.
\end{align*}
We can simplify the notation by replacing the summation over multiple $r_k, s_k$ by a sum over a single pair $r, s$ and grouping together the coefficients of the power $t^s$. To do this, we introduce the pseudo-cumulants $\alpha^*_s$ such that
\begin{equation} \label{eq-char-expansion}
    \zeta(t) = 
    \sum_{j=0}^\infty 
    \sum_{s \in S(j)}
    \alpha^*_s \frac{\xi(t) i^{j} t^{s}}{j!}.
\end{equation}
One sees that for $s \in S(j)$, the pseudo-cumulant $\alpha^*_s$ is a sum over products of the form $\alpha_{s_1}\ldots\alpha_{s_l}$ where $s_1 \in S(j_1), \ldots, s_l \in S(j_l)$ such that $j_1 + \ldots + j_l = j$ and the indices in $s$ and $s_1, \ldots, s_l$ match. For instance, for $j = 1, 2, 3$, the pseudo-cumulants $\alpha^*$ are of the form
\begin{align*}
    \alpha^*_{(k)} &= \alpha_{(k)} \\
    \alpha^*_{(k, l)} &= \alpha_{(k, l)} + \alpha_{(k)}\alpha_{(l)}\\
    \alpha^*_{(k, l, m)} &= \alpha_{(k, l, m)} + \alpha_{(k, l)}\alpha_{(m)} + \alpha_{(k)}\alpha_{(l)}\alpha_{(m)},
\end{align*}
where the exact coefficient in front of the $\alpha$ terms are not relevant and ignored for conciseness. Coming back, by Lemma \ref{lemma-fourier-derivative}, we recognize the Fourier transform of derivatives of the density $q$ of $Q$
\begin{equation*}
    \xi(t) (-i)^{j} t^s  = \F \left[ D^s q \right],
\end{equation*}
which allows us to retrieve the density of $P$ by Fourier inversion
\begin{align}\label{eq-edge-abstract}
    f(x) = \F^{-1}\left[\xi\right] &= 
    \sum_{j=0}^\infty 
    \sum_{s \in S(j)}
    \alpha^*_s \frac{(-1)^j D^s q(x)}{j!}\nonumber\\
    &= 
    q(x) \left\{ 1 + \sum_{j=1}^\infty 
    \sum_{s \in S(j)}
    \alpha^*_s \frac{(-1)^j D^s q(x)}{j! q(x)}\right\}
\end{align}
A convenient choice for $Q$ is the multivariate Normal distribution $\mathcal{N}_p(0, \mathbb{1}_p)$. Then, we have that the cumulants of $P$ and $Q$ of order $k=1,2$ of the two distributions match, implying $\alpha_s = 0$ for any $s \in S(k), k=1,2$. Since the pseudo-cumulants $\alpha^*$ are composed of sums and products of the coefficients $\alpha$, this also implies that the pseudo-cumulants of order $k=1,2$ are 0 as well. Using this in (\ref{eq-edge-abstract}), we obtain
\begin{align}
    f(x) 
    &= \phi(x) \left\{ 1 + \sum_{j=3}^\infty 
    \sum_{s \in S(j)}
    \alpha^*_s \frac{(-1)^j D^s \phi(x)}{j! \phi(x)}\right\} \nonumber \\
    &= \phi(x) \left\{
        1 + \sum_{j=3}^\infty  \sum_{s \in S(j)} \frac{1}{j!}\alpha^*_s h_s(x)
    \right\}, \label{eq-edgeworth-full}
\end{align} 
where $h_s(\cdot)$ are a multivariate generalization of the Hermite polynomials given by
\begin{equation*}
    h_s(x) = (-1)^j \frac{D^s \phi(x)}{\phi(x)}.
\end{equation*}
\
Consider now applying this transformation to the standardized sum $Y = n^{-1/2}\sum_{i=1}^n X_i$ where $X_i \simiid P$. Then for any $s \in S(k)$, using properties of cumulants given in Lemma \ref{lem-cumulants-props}, the $s$-cumulants of $Y$ are given by 
\begin{equation*}
    \kappa_s(Y) = n^{1-k/2} \kappa_s(X_1) = o(n^{1-k/2}).
\end{equation*}
\
We can form the \textit{Edgeworth series} of order $k$, called $e_k(y; \kappa(X))$, by discarding terms of order higher than $o(n^{1-k/2})$ in Equation (\ref{eq-edgeworth-full}), giving
\begin{equation} \label{eq-edgeworth}
    f_Y(y) = e_k(y; \kappa(X)) + o(n^{(1-k)/2}).
\end{equation}

Note that this statement can be slightly refined, which will be useful later. After truncating Equation (\ref{eq-edgeworth-full}), the density $f$ can be decomposed in
\begin{equation} \label{eq-edge-polynomial}
    f(y) = \phi(y)\left\{1 + P_k(y; \kappa(X)) + o(n^{(1-k)/2})\right\},
\end{equation}
where $P_k(\cdot; \kappa(X))$ is the polynomial part of the Edgeworth approximation.

\begin{example} \label{ex-edgeworth-1d}
    Consider a random variable $X \in \R$ with cumulants $\kappa(X) = (\kappa_1, \kappa_2, \ldots)$ such that $\E{X} = 0$ and $\V[X] = 1$. In one dimension, derivatives can only be taken with respect to a single variable and Equation (\ref{eq-edgeworth-full}) becomes
    \begin{equation*}
        \phi(x) \left\{
            1 + \sum_{j=3}^\infty  \frac{1}{j!}\alpha^*_j h_j(x)
        \right\}.
    \end{equation*}
    Let again $Y$ be a standardized sum of $n$ independent copies of $X$. To construct the Edgeworth approximation of order $k = 4$ to the density of $Y$, we truncate the above equation to only keep terms of size at least $o(n^{-1})$, that is we keep terms of size $o(n^{-1/2})$ and $o(n^{-1})$. As mentioned ealier, each cumulant $\kappa_k(Y)$ is of order $o(n^{1-k/2})$, hence, the following products of cumulants can result in a term of the desired orders
    \begin{align*}
        \kappa_3(Y) = \frac{\kappa_3}{\sqrt{n}} && \kappa_3(Y)\kappa_3(Y) = \frac{\kappa_3^2}{n} && \kappa_4(Y) = \frac{\kappa_4}{n}.
    \end{align*}
    Finding the right coefficients of each of these terms from the definition of the corresponding $\alpha^*$, we obtain the following expression of the Edgeworth series
    \begin{equation} \label{eq-edgeworth-1d-4}
        e_4(y; \kappa(X)) = \frac{1}{\sqrt{1\pi}}\expf{-\frac{y^2}{2}}\left\{1 + \frac{\kappa_3 H_3(y)}{6\sqrt{n}} + \frac{3\kappa_4 H_4(y) + \kappa_3^2 H_6(y)}{72 n}\right\}.
    \end{equation}
\end{example}

\paragraph{} While the argument provided above for the definition of the Edgeworth series is not suffenciently rigorous to prove Equation (\ref{eq-edgeworth}), we now show that the Edgeworth series $e_k(y; \kappa(X))$ indeed approximates the density of a standardized sum with an error of $o(n^{(1-k)/2})$.

\begin{remark} \label{rem-centering}
    The initial assumption of having a mean of 0 and covariance matrix equal to the identity is not a restriction of the approach. Indeed, if $X$ has a mean $\mu$ and covariance matrix $\Sigma$, the Edgeworth series $e_k(\cdot; \kappa(Z))$ can be constructed for the random variable $Z = \Sigma^{-1/2}(X - \mu)$ and used to construct an approximation $e_k(\cdot; \kappa(X))$ of the density of $n^{-1/2} \sum_{i=1}^n X_i$ via
    \begin{equation*}
        e_k(s; \kappa(X)) = \abs{\Sigma}^{-1/2} e_k(\Sigma^{-1/2}(s - \sqrt{n}\mu); \kappa(Z)).
    \end{equation*}

    In the rest of this thesis, we will use the Edgeworth expansion to approximate the density of random variables which are not necessarily centered or have a unit covariance. In this case, we will implicitly make use of the change of variable formula mentioned in this remark. 
\end{remark}

\begin{theorem} \label{thm-edgeworth}
    Let $P$ be a distribution and $k \in \N_{\geq 2}$ such that all cumulants $\kappa$ of $P$ of order up to $k$ exist. Let $n \in \N$ and $X_1, \ldots, X_n \simiid P$ and $Y$ be the standardized sum
    \begin{equation*}
        Y = n^{-1/2}\sum_{i=1}^n X_i.
    \end{equation*}
    Assume that the characteristic function $\zeta$ of $P$ is $L^q(\Rp)$ for some $q \in [1, n)$. Then the
    
    Let $e_k(y; \kappa)$ be the Edgeworth series, constructed by only keeping terms order up to $O(n^{1-k/2})$ in Equation (\ref{eq-edgeworth-full}). Then, if the density $f_Y$ of $Y$ exists, $e_k(y; \kappa)$ approximates $f_Y$ with a uniform error of order $o(n^{(1 - k)/2})$.
\end{theorem}
\begin{proof}
    Without loss of generality, we assume that $X_i$ has a mean equal to 0 and a covariance matrix equal to the identity, see Remark \ref{rem-centering}. Let $\xi$ be the Fourier transform of $e_k(\cdot; \kappa)$, then by Corollay \ref{corr-fourier-inv}, we can bound the the absolute difference between $f_Y$ and $e_k(\cdot; \kappa)$ as
    \begin{equation*}
        |f_Y(y) - e_k(y; \kappa)| \leq (2\pi)^{-p}\intRp{t}{|\zeta(tn^{-1/2})^n - \xi(t)|},
    \end{equation*}
    where $\zeta(tn^{-1/2})^n$ is the characteristic function of $Y$. Since both $\zeta$ and $\xi$ are $L^1(\Rp)$, the integral is well defined and provides a valid upper bound. We proceed by splitting the range of integration in two parts: one parts in which $t$ is small such that Theorem \ref{thm-edge-inv-tech} can be used, and the rest of the integral will be handled separately.

    By construction of the Edgeworth series, the Fourier transform of $e_k(\cdot; \kappa)$ corresponds to the function given in Equation (\ref{eq-edgeworth-fourier}) of Theorem \ref{thm-edge-inv-tech}. Hence, for any $\varepsilon$, there is a $\delta$ such that (\ref{eq-edge-bound}) holds and we can bound the \textit{small $t$} part of the integral as
    \begin{align*}
        \int_{B_2(\delta\sqrt{n})} &|\zeta(tn^{-1/2})^n - \xi(t)| \d t\\
        &\leq (2\pi)^{-p}\int_{B_2(\delta\sqrt{n})}\expf{-\frac{1}{4}\norm{t}_2^2}\left[ \frac{\epsilon\norm{t}_2^k}{n^{k/2-1}} + \frac{C_0^{k-1}\norm{t}_2^{3(k-1)}}{(k-1)!n^{k/2-1/2}} \right] \d t \\
        &\leq \frac{\epsilon C_1}{n^{k/2-1}}\expec{T}{\norm{T}^k_2} + \frac{C_2^{k-1}}{(k-1)!n^{k/2-1/2}}\expec{T}{\norm{T}_2^{3(k-1)}}
        = o(n^{(1-k)/2}),
    \end{align*}
    in which $T \sim N(0, 2\mathbb{1}_p)$ and $C_0, C_1, C_2 \in \R$ are constants that do not depend on $n$. 

    For the remaining part of the integral, where $\norm{t}_2 \geq \delta \sqrt{n}$, we bound the integral by the triangle inequality and consider each term separately,
    \begin{align*}
        \int_{\Rp \setminus B_2(\delta\sqrt{n})} &|\zeta(tn^{-1/2})^n - \xi(t)| \d t \\
        &\leq \int_{\Rp \setminus B_2(\delta\sqrt{n})} |\xi(t)| \d t + \int_{\Rp \setminus B_2(\delta\sqrt{n})} |\zeta(tn^{-1/2})^n| \d t\\
        &= I_1 + I_2.
    \end{align*}
    By construction of $\xi$, the integral $I_1$ has an exponential decay, faster than the $o(n^{(1 - k)/2})$ we are trying to show. As for the integral $I_2$, using that $|\zeta(t)| < 1$ for $t \neq 0$ and $|\zeta(t)| \rightarrow 0$ for $n \rightarrow \infty$, there exists a $a \in (0, 1)$ such that for $n$ large enough if $\norm{t}_2 \geq \delta\sqrt{n}$ then $|\zeta(tn^{-1/2})| \leq a$. Furthermore, by assumption of the existence of $f_Y$ and Lemma \ref{lem-char-integrable-convolution}, there exists a $q > 1$ such that $\zeta^n \in L^q(\Rp)$. Thus,
    \begin{equation*}
        I_2
        \leq a^{n-q} \int_{\Rp \setminus B_2(\delta\sqrt{n})} |\zeta(tn^{-1/2})|^q \d t 
        \leq a^{n-q}\sqrt{n}\intRp{t}{|\zeta(t)|^q} 
        = o(\sqrt{n}a^n) = o(n^{(1-k)/2}),
    \end{equation*}
    which concludes he proof.
\end{proof}

\begin{remark} \label{rem-edge-mean}
    Note that one can easily show that for any index tuple $s \in S(k)$ with $k$ odd, 0 is a root of the generalized Hermite polynomial $h_s$. 
    Furthermore, by the development of Equation (\ref{eq-edgeworth-full}), the coefficient of each Hermite polynomial $h_s, s \in S(k)$ contains terms of order $O(n^{1-k'/2})$ where $k$ and $k'$ have the same parity and $k' \leq k$.

    Together with Remarks \ref{rem-centering}, this shows that the polynomial part of the Edgeworth series evaluated at the mean of the approximated distribution is a polynomial in $n^{-1}$ instead of a polynomial in $n^{-1/2}$ since terms of odd powers are zero. Another consequence of this is that the error of the Edgeworth approximation of order $k$ is $o(n^{\floor{(1-k)/2}})$. 
    
    For instance, if $e_4(\cdot; \kappa(X))$ is Edgeworth expansion of order $4$ from Example \ref{ex-edgeworth-1d}, we have
    \begin{equation*}
        f_Y(y) = e_4(y; \kappa(X)) + \begin{cases}
            o(n^{-2}) &\text{if } y = 0\\
            o(n^{-3/2}) &\text{otherwise}.
        \end{cases} 
    \end{equation*} 
\end{remark}


% \section{Properties of characteristic functions}

Throughout this chapter, we consider a family of multivariate continuous distributions $\{ P_\theta \}$ indexed by a parameter $\theta \in \Theta$.



\begin{lemma} \label{lemma-series}
    For any $u, v \in \C$ and any $l \in \mathbb{N}$ the following inequality holds
    \begin{equation}
        \abs{\exp{u} - \sum_{k=0}^l \frac{v^k}{k!}} \leq \max \left\{ \exp{|u|}, \exp{|v|} \right\}\left(\abs{u - v} + \abs{\frac{v^{l+1}}{(l+1)!}}\right).
    \end{equation}
\end{lemma}

\begin{proof}
    By the triangle inequality,
    \begin{equation*}
        \abs{\exp{u} - \sum_{k=0}^l \frac{v^k}{k!}} \leq \abs{\exp{u} - \exp{v}} + \abs{\sum_{k=l+1}^\infty \frac{v^k}{k!}}.
    \end{equation*}
    Starting with the second term on the right hand side, we have
    \begin{align*}
        \abs{\sum_{k=l+1}^\infty \frac{v^k}{k!}} 
        & \leq \sum_{k=l+1}^\infty \abs{\frac{v^k}{k!}} 
        = \abs{\frac{v^{l+1}}{(l+1)!}} \sum_{k=0}^\infty \abs{v^k} \frac{(l+1)!}{(k + l + 1)!} \\
        & \leq \abs{\frac{v^{l+1}}{(l+1)!}} \sum_{k=0}^\infty \abs{\frac{v^k}{k!}}
        = \abs{\frac{v^{l+1}}{(l+1)!}} \exp v\\
        &\leq \abs{\frac{v^{l+1}}{(l+1)!}} \max \left\{ \exp{|u|}, \exp{|v|} \right\}.
    \end{align*}
    Furthermore, by Taylor's theorem there exists a point $z \in \C$ lying on the straight line between $u$ and $v$ such that
    \begin{equation*}
        \exp u - \exp v = \abs{u - v} \exp z.
    \end{equation*}
    Taking absolute values and by convexity of the exponential function, we find the following bound
    \begin{equation*}
        \abs{\exp u - \exp v} = \abs{u - v} \exp |z| \leq \abs{u - v} \max \left\{ \exp{|u|}, \exp{|v|} \right\}.
    \end{equation*}
    Combining the two bounds found previously completes the proof of the lemma.
\end{proof}


\begin{theorem} \label{thm-edge-inv-tech}
    Let $\zeta$ be the characteristic function of a random vector $X \in \Rp$ and let $n \in \N$. Assume that all cumulants of $X$ of order up to $k \in \N$ exist and that the second cumulants of $X$ satisfy $\kappa^{(i, j)} = \delta_{ij}$ for all $i, j = 1, \ldots, p$. Let
    \begin{equation} \label{eq-edgeworth-fourier}
        \xi(t) = \expf{-\frac{1}{2}\norm{t}_2^2}\sum_{l=0}^{k-2} \frac{1}{l!} \left[ 1 + \sum_{m=3}^k \sum_{s \in S(m)} \frac{i^m\kappa_s t^s}{n^{m/2-1}m!} \right]^l.
    \end{equation}
    Then for every $\epsilon > 0$ there exists a $\delta > 0$ and a constant $C_p$ dependent on the dimension of $X$ such that 
    \begin{equation}\label{eq-edge-bound}
        \abs{\zeta(t n^{-1/2})^n - \xi(t)} \leq \expf{-\frac{1}{4}\norm{t}_2^2}\left[ \frac{\epsilon\norm{t}_2^k}{n^{k/2-1}} + \frac{C_p^{k-1}\norm{t}_2^{3(k-1)}}{(k-1)!n^{k/2-1/2}} \right]
    \end{equation}
    holds for all $t \in \Rp$ with $\norm{t}_2 < \delta\sqrt{n}$.
\end{theorem}
\begin{proof}
    The idea of this proof is to rewrite the left hand side of the Equation (\ref{eq-edge-bound}) to be able to use Lemma \ref{lemma-series} and find suitable upper bounds on the remaining quantities. To that end, we define $u(t) = n u^*(t n^{-1/2})$ and $v(t) = nv^*(t n^{-1/2})$ where $u^*(t) = \log \zeta(t) + \frac{1}{2}\norm{t}_2^2$ and 
    \begin{equation*}
        v^*(t) = \sum_{m=3}^k \sum_{s \in S(m)} \frac{i^m\kappa_s t^s}{m!}.
    \end{equation*}
    We can then rewrite
    \begin{equation*}
        \zeta(t n^{-1/2})^n = \expf{n\left[\log \zeta(t n^{-1/2})\right]} = \expf{-\frac{1}{2}\norm{t}_2^2}\exp u(t)
    \end{equation*}
    and
    \begin{equation*}
        \xi(t) = \expf{-\frac{1}{2}\norm{t}_2^2}\sum_{l=0}^{k-2} \frac{v(t)^l}{l!}.
    \end{equation*}
    By Lemma \ref{lemma-series}, we can bound the left hand side of Equation (\ref{eq-edge-bound}) and have
    \begin{align*}
        &\abs{\zeta(t n^{-1/2})^n - \xi(t)} 
        = \expf{-\frac{1}{2}\norm{t}_2^2}\abs{\exp u(t) - \sum_{l=0}^{k-2} \frac{v(t)^l}{l!}}\\
        &\leq \expf{-\frac{1}{2}\norm{t}_2^2} \max \left\{ \exp{|u(t)|}, \exp{|v(t)|} \right\}\left(\abs{u(t) - v(t)} + \frac{|v(t)|^{k-1}}{(k-1)!}\right)
    \end{align*}
    We now continue to find suitable bounds on the resulting quantities. First note that both $u$ and $v$ have continuous derivatives in 0 of order up to $k$. Starting with $u^*$, let $s \in S(m)$ for $3 \leq m < k$, then
    \begin{align*}
        D^s u^*(0)
        &= \ddx{t_{s_1}}\ldots\ddx{t_{s_m}} \log \zeta(t) + \frac{1}{2}\norm{t}_2^2\bigg|_{t=0}\\
        &=  \ddx{t_{s_1}}\ldots\ddx{t_{s_m}} \log \zeta(t)\bigg|_{t=0}
            +
            \ddx{t_{s_1}}\ldots\ddx{t_{s_m}} \frac{1}{2}\norm{t}_2^2\bigg|_{t=0}
        \\
        &= (-i)^{-m}\kappa_s = i^m \kappa_s ,
    \end{align*}
    where the derivative of the 2-norm of $t$ is 0 because $|s| = m \geq 3$. Furthermore, we can compute the same derivatives of $v^*$,
    \begin{align*}
        \ddx{t_{s_1}}\ldots\ddx{t_{s_m}} v^*(t) \bigg|_{t=0}
        &= \ddx{t_{s_1}}\ldots\ddx{t_{s_m}} \sum_{m'=3}^k \sum_{s' \in S(m')} \frac{i^{m'}\kappa^{s'}t_{s'_1}\ldots t_{s'_{m'}}}{m'!} \bigg|_{t=0} \\
        &= \sum_{m'=3}^k \sum_{s' \in S(m')} \frac{i^{m'}\kappa^{s'}}{m'!} \ddx{t_{s_1}}\ldots\ddx{t_{s_m}}t_{s'_1}\ldots t_{s'_{m'}} \bigg|_{t=0}.
    \end{align*}
    The term $\ddx{t_{s_1}}\ldots\ddx{t_{s_m}}t_{s'_1}\ldots t_{s'_{m'}} |_{t=0} = 1$ if and only if $s'$ is a permutation of $s$, otherwise 0. Hence
    \begin{equation*}
        \ddx{t_{s_1}}\ldots\ddx{t_{s_m}} v^*(t) \bigg|_{t=0} 
        = \sum_{s' \in S(m)} \frac{i^m\kappa^{s'}}{m!}
        = i^m\kappa_s,
    \end{equation*}
    where the last equality holds since $\kappa_s = \kappa^{s'}$ for all permutation $s'$ of $s$. 
    This shows that all derivatives of order up to $k$ of $u^* - v^*$ (and hence also $u - v$) exist in 0 and are all equal to 0. Therefore, there exists a $\delta > 0$ such that for all $t \in \Rp$ with $\norm{t}_2 \leq \delta$
    \begin{equation*}
        \abs{u^*(t) - v^*(t)} \leq \epsilon\norm{t}_2^k
    \end{equation*}
    and if $\norm{t} \leq \delta \sqrt{n}$, this bound yields
    \begin{equation*}
        \abs{u(t) - v(t)} = n\abs{u^*(t n^{-1/2}) - v(t n^{-1/2})} \leq n\epsilon\norm{n^{-1/2}t}_2^k = n^{1-k/2}\norm{t}_2^k\epsilon.
    \end{equation*}
    Furthermore, choose $\delta$ small enough such that $\abs{u^*(t)} < \norm{t}_2^2/4$ for $\norm{t}_2 \leq \delta$. Then for $\norm{t}_2 \leq \delta\sqrt{n}$ we have
    \begin{equation*}
        \abs{u(t)} = n\abs{u^*(t n^{-1/2})} \leq n \norm{t n^{-1/2}}_2^2/4 = \norm{t}_2^2/4.
    \end{equation*}
    We observe that all derivatives of $v^*$ in 0 of first and second order are equal to 0 and that the third derivatives of $v^*$ are bounded. By Taylor's theorem, this allows us to find the following bound for all $\norm{t}_2 \leq \delta \sqrt{n}$
    \begin{equation*}
        |v(t)| = |nv^*(t n^{-1/2})| < C_p n \norm{t n^{-1/2}}_2^3 = C_p \norm{t}_2^3 n^{-1/2}
    \end{equation*}
    where
    \begin{equation*}
        C_p = \sup_{\substack{\norm{t}_2 \leq \delta, \\ s \in S(3)}} p^3\abs{ D^sv^*(t) }.
    \end{equation*}
    Hence, for a $\delta$ small enough and $t \leq \delta\sqrt{n}$ we have
    \begin{align*}
        &\abs{\zeta(t n^{-1/2})^n - \xi(t)} \\
        &\leq \expf{-\frac{1}{2}\norm{t}_2^2} \max \left\{ \exp{|u(t)|}, \exp{|v(t)|} \right\}\left(\abs{u(t) - v(t)} + \frac{|v(t)|^{k-1}}{(k-1)!}\right)\\
        &\leq \expf{-\frac{1}{2}\norm{t}_2^2}
            \max \left\{ 
                \expf{\frac{1}{4}\norm{t}_2^2},
                \expf{C \norm{t}_2^3 n^{-1/2}}
            \right\}\\
        & \times \left(
                \frac{\epsilon\norm{t}_2^k}{n^{k/2-1}}
                +
                \frac{C_p^{k-1} \norm{t}_2^{3(k-1)} }{n^{k/2-1/2} k!}
              \right)\\
        & \leq \expf{-\frac{1}{4}\norm{t}_2^2}\left[ \frac{\epsilon\norm{t}_2^k}{n^{k/2-1}} + \frac{C_p^{k-1}\norm{t}_2^{3(k-1)}}{(k-1)!n^{k/2-1/2}} \right].
    \end{align*}
\end{proof}

Note that this proof doesn't use the fact that $\zeta$ is a characteristic function. Indeed, this theorem can be proven in a more general setting without any changes to the statement of the theorem or the proof itself.