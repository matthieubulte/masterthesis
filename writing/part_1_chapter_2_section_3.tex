We now introduce another approach based on the Edgeworth series to create accurate density approximations that avoids some of the issues of the Edgeworth series that we have demonstrated in the previous section.

We now introduce the idea of \textit{exponential tilting}. Consider a random random variable $X \sim \Rp$ with cumulant generative function $K$ and density $f$. We introduce the exponential family $\mathcal{T}_P = \{ P_\gamma \}_{\gamma \in \Rp}$ where each $P_\gamma \in \mathcal{T}_P$ is characterized by its density function $f_\gamma$ given by
\begin{equation*}
    f_\gamma(x) = f(x)\expf{\gamma^\top x - K(\gamma)}.
\end{equation*}
Note that by the definition of the cumulant generating function, $K(\gamma)$ is the right normalization factor for $f_\gamma$ and hence $f_\gamma$ integrates to 1 and is a valid density function. Furthermore, the original distribution $P$ is an element of $\mathcal{T}_P$ with $P = P_0$. Given two distributions in $\mathcal{T}_P$, their densities only differ by the ratio of $\expf{\gamma^\top x - K(\gamma)}$. Since the following holds for any $\gamma \in \Rp$
\begin{equation} \label{eq-saddlepoint-original}
    f(x) = f_\gamma(x)\expf{K(\gamma) - \gamma^\top x},
\end{equation}
we can construct an approximation of $f$ by choosing $\gamma$ such that $f_\gamma$ can be accurately approximated.

Let us now consider a distribution $P$ with cumulant generating function $K$. We wish to use the previous argumentation to approximate the density $f$ of the mean $S$ of $n$ i.i.d.\,random variables distributed according to $P$. The cumulant generating function of $S$ in Equation (\ref{eq-saddlepoint-original}), we get
\begin{equation*}
    f(s) = f_\gamma(s)\expf{nK(\gamma / n) - \gamma^\top s}.
\end{equation*}
We can now get back to the question of the choice of $\gamma$. We are interested in choosing $\gamma$ such that the Edgeworth approximation of $\bar f_\gamma(x)$ is accurate. As seen in Remark \ref{rem-edge-mean}, the second order Edgeworth approximation of odd order gains half an order of accuracy when evaluated at the mean of the distribution. Since $\gamma$ can be chosen freely and differently for each value $s$ at which the density $f(s)$ is evaluated, we can choose $\gamma$ such that $s$ is the mean of the distribution $P_\gamma$. Given $\gamma \in \Rp$, the cumulant generating function of $P_\gamma$ is equal to
\begin{equation*}
    K_\gamma(t) = n\left[K((\gamma + t)/n) - K(\gamma/n)\right].
\end{equation*}
One sees that derivatives of the cumulant generating function $K_\gamma$ can be expressed in terms of the cumulant generating function $K$ by
\begin{equation} \label{eq-deriv-Kgamma}
    K_\gamma^s(t) = n^{1 - k} K((\gamma + t) / n),
\end{equation}
for any $s \in S(k)$. Thus, using a well-known property of exponential families, the expectated value of the distribution $P_\gamma$ can be expressed in terms of the gradient of the cumulant generating function of $X$,
\begin{equation*}
    \expec{S \sim P_\gamma}{S} = K'(\gamma / n).
\end{equation*}
For any $s \in \Rp$, we can now find a distribution $P_{\hat\gamma_s} \in \mathcal{T}_P$ with mean $s$ by solving
\begin{equation} \label{eq-saddlepoint}
    K'(\hat\gamma_s / n) = s.
\end{equation}
We choose to call the solution of this equation $\hat\gamma_s$ to emphasize the fact that instead of choosing one unique $\gamma$ and then construct an approximation of the density of $P_\gamma$ over $\Rp$, we find a different $\hat\gamma_s$ at each $s \in \Rp$ such that the Edgeworth approximation of the density of $P_{\hat\gamma_s}$ is accurate in $s$. Note that if $\hat\gamma_s$ solves (\ref{eq-saddlepoint}), it is also the maximum likelihood estimator of $\gamma$ within the model $\mathcal{T}_P$. We can then construct the Edgeworth approximation $e_k(s; \kappa(\hat\gamma_s))$ to the density $f_{\hat\gamma_s}$ with
\begin{equation*}
    f_{\hat\gamma_s}(s) = e_k(s; \kappa(\hat\gamma_s)) + o(n^{1-k/2}).
\end{equation*}
Replacing this in the expression of $f$ in terms of $f_{\hat\gamma_s}$ gives
\begin{align}
    f(s) &= \expf{nK(\hat\gamma_s / n) - \hat\gamma_s^\top s}\left[e_k(s; \kappa(\hat\gamma_s)) + o(n^{1-k/2})\right]\nonumber\\
    &= g_k(s; K)\left[1 + o(n^{1-k/2})\right]. \label{eq-saddle-exp}
\end{align}
A special case of particular interest is for $k = 3$. In this case, the Edgeworth approximation of $P_{\hat\gamma_s}$ is equal to its normal approximation and is given by
\begin{equation*}
    e_3(s; \kappa(\hat\gamma_s)) = (2\pi)^{-p/2}|\Sigma_{\hat\gamma_s}|^{-1/2}\expf{-\frac{1}{2}(s - \mu_{\hat\gamma_s})^\top \Sigma_{\hat\gamma_s}^{-1/2}(s - \mu_{\hat\gamma_s}) },
\end{equation*}
where $\mu_{\hat\gamma_s}$ and $\Sigma_{\hat\gamma_s}$ are the mean and covariance of $P_{\hat\gamma_s}$. By the choice of $\hat\gamma_s$, we have that $\mu_{\hat\gamma_s} = s$ and hence the term in the exponential is equal to zero. Furthermore, the covariance matrix $\Sigma_{\hat\gamma_s}$ is equal to $K_{\hat\gamma_s}''(\hat\gamma_s)$, the Hessian of $K_{\hat\gamma_s}$ evaluated at $\hat\gamma_s$. From Equation (\ref{eq-deriv-Kgamma}), $K''_{\hat\gamma_s}(\hat\gamma_s) = n^{-1} K''(\hat\gamma_s/n)$ and we get 
\begin{equation} \label{eq-saddle-3}
    g_3(s; K) = \left(\frac{n}{2\pi}\right)^{p/2}|K''(\hat\gamma_s/n)|^{-1/2} \expf{nK(\hat\gamma_s / n) - \hat\gamma_s^\top s}.
\end{equation}
We call this approximation the \textit{Saddlepoint approximation}. We can see that the Saddlepoint approximation is always positive, and by Remark \ref{rem-hermite-odd} and the choice of $\hat\gamma_s$,
\begin{align}
    f(s) 
    &= g_3(s; K)(1 + o(n^{-1}))\nonumber\\
    &= g_3(s; K)\left[1 + \frac{b(\hat\gamma_s / n)}{2n} + o(n^{-2})\right], \label{eq-saddle-o2}
\end{align}
where $b(\hat\gamma_s / n)$ is the coefficient of the terms of order $O(n^{-1})$ in the Edgeworth expansion of the tilted density. We justify this expansion error in the following theorem.

\begin{theorem}
    Let $P$ be a distribution with cumulant genrating function $K$ and $k \in \N_{\geq 2}$ such that all cumulants of $P$ of order up to $k$ exist. Suppose that $K$ is defined in an open neighborhood $U$ of 0 and that for every $s \in \Rp$, Equation (\ref{eq-saddlepoint}) has a unique solution $\hat\gamma_s \in U$. 

    Let $n \in \N$ and $X_1, \ldots, X_n \simiid P$ and $S$ be the mean
    \begin{equation*}
        S = n^{-1} \sum_{i=1}^n X_i.
    \end{equation*}

    Then the Saddlepoint approximation $g_k(\cdot; K)$ given in Equation (\ref{eq-saddle-exp}) approximates $f_S$, the density of $S$, with an relative error of order $o(n^{1-\ceil{k/2}})$.
\end{theorem}

While the Saddlepoint approximation shows many advantages over the Edgeworth approximation, it is important to note that the Saddlepoint approximation uses information from the complete cumulant generating function of the approximated density. The Edgeworth approximation on the other hand only uses the first $k$ moments of the distributions, which are evaluations of derivatives of the cumulant generating function in 0. 

\begin{figure}[h]
    \textbf{Approximation error of $\Gamma(2,1)$ standardized sums with $n=10$}
    \centering
    \subfloat{
        \includegraphics[width=8cm]{saddlepoint_and_edgeworth_err_abs_gamma21_10_terms.eps} 
    }
    \subfloat{
        \includegraphics[width=8cm]{saddlepoint_and_edgeworth_err_rel_gamma21_10_terms.eps} 
    }
    \caption{Study of the approximation error of the Saddlepoint approximation on a standardized sum of $n=10$ of $\Gamma(2, 1)$ random variables. Both panel exposes properties studied of the Saddlepoint approximation: the accurate rellative error, the gain in order of approximation and the uniform relative error of the approximation for sums of Gamma random variables.}
    \label{fig-saddlepoint-err}
\end{figure}

\begin{example} \label{ex-gamma-saddle}
    Continuing Example \ref{ex-gamma-edge}, we can analyze the behaviour of the Saddlpoint approximation to the mean $Y = n^{-1}\sum_{i=1}^n X_i \in \R_+$ where $X_1, \ldots, X_n \simiid \Gamma(p, \lambda)$. The cumulant generating function of the $\Gamma(n, p)$ distribution is $K(t) = p\logf{\lambda} - p\logf{\lambda - t}$ and its first derivative is $K'(t) = p / (\lambda - t)$. For any $s \in \R_+$, the Saddlepoint $\hat\gamma_s$ is given by the solution to the Saddlepoint Equation (\ref{eq-saddlepoint}), which here becomes
    \begin{equation*}
        \frac{p}{\lambda - \hat\gamma_s/n} = s \Rightarrow \hat\gamma_s = n\left(\lambda - \frac{p}{s}\right).
    \end{equation*}
    In Figure \ref{fig-saddlepoint-err}, we demonstrate how the Saddlepoint approximation of order 3 compares to the Edgeworth approximation when approximating a standardized sum of $n$ random variables independently distributed according to $\Gamma(2, 1)$. Since the standardized sum can be obtained by multiplying the mean by a factor of $\sqrt{n}$, the Saddlepoint approximation is easily adapted by change of variable. Both panels show accurate approximation properties both in terms of relative and absolute error.
    
    In this example, it is also interesting to examine the concrete form of the Saddlepoint approximation $g_3$. Replacing the relevant quantities in Equation (\ref{eq-saddle-3}), we obtain that the Saddlepoint approximation is
    \begin{align*}
        g_3(s; K) &= \sqrt{\frac{n}{2\pi K''(\lambda - \frac{p}{s})}} \expf{nK\left(\lambda - \frac{p}{s}\right) - n\left(\lambda - \frac{p}{s}\right)s}\\
        &= \sqrt{\frac{n}{2\pi s^2/p}} \expf{n\left(p\logf{\lambda} - p\logf{p/s}\right) - ns\lambda + np}\\
        &= (n\lambda)^{np}s^{np-1}\expf{-sn\lambda} \times \frac{(np)^{1/2-np}\expf{np}}{\sqrt{2\pi}}.
    \end{align*}
    Consider now the Stirling's formula for the gamma function
    \begin{equation*}
        \Gamma(z) \approx \sqrt{2\pi}z^{z-1/2}\expf{-z}.
    \end{equation*}
    We recognize that the second term in the expression of $g_3(s; K)$ corresponds to the inverse of the Stirling's approximation to $\Gamma(np)$. The Saddlepoint approximation to the density of the mean of $\Gamma(p, \lambda)$ variables thus corresponds to the density of the true distribution $\Gamma(np, n\lambda)$ of the mean, where the gamma function has been replaced by the Stirling's approximation. This has the interesting consequence that the relative error of the Saddlepoint approximation does not depend on $s$, the point at which the density is evaluated, but rather only depends on $n$. This behaviour is also exposed in Figure \ref{fig-saddlepoint-err} where the relative error of the Saddlepoint approximation is a straight horizontal line. Daniels \cite{daniels1954saddlepoint} characterizes the class of distributions for which the uniform relative approximation error holds.
\end{example}

Consider now applying the Saddlepoint approximation to an exponential family $\mathcal{P} = \left\{P_\theta\right\}_{\theta}$ with natural parameter $\theta \in \Rp$. The density $f_\theta$ of $P_\theta \in \mathcal{P}$ is given by
\begin{equation*}
    f_\theta(x) = \expf{\theta^\top T(x) - \mathcal{H}(\theta) - \mathcal{G}(x)}.
\end{equation*}
Given a random sample $x = (x_1, \ldots, x_n)$ of $P_\theta$, the loglikelihood function is given by
\begin{equation*}
    \ell(\theta; x) = \theta^\top \sum_{i=1}^n T(x_i) - n \mathcal{H}(\theta) = n\left[\bar t - \mathcal{H}(\theta)\right],
\end{equation*}
where $\bar t$ is the sample average of the sufficient statistic, $\bar t = n^{-1}\sum_{i=1}^n T(x_i)$. Hence, the maximum likelihood estimator of $\theta$ is the value $\hat\theta_{\bar x} \in \Rp$ satisfying the score equation
\begin{equation} \label{eq-score-expfam}
    \mathcal{H}'(\hat\theta_{\bar t}) = \bar t.
\end{equation}
For simplicity, we will assume that $\mathcal{H}'$ is one-to-one to ensure that (\ref{eq-score-expfam}) has a unique solution $\hat\theta_{\bar t}$. In the exponential family $\mathcal{P}$, it can be shown that the cumulant generating function of any member $P_\theta \in \mathcal{P}$ is given by $K_\theta(t) = \mathcal{H}(\theta + t) - \mathcal{H}(\theta)$ and thus $K'_\theta(t) = \mathcal{H}'(\theta + t)$. Using the cumulant generating function in the score equation gives
\begin{equation*}
    K'_\theta(\hat\theta_{\bar t} - \theta) = \bar t.
\end{equation*}
Considering the Saddlepoint Equation given in (\ref{eq-saddlepoint}), we notice that the  parameter $\hat\gamma_{\bar t}$ of the tilted family nearly correspounds to the maximum likelihood estimator $\hat\theta$ with
\begin{equation*}
    \hat\gamma_{\bar t}/n = \hat\theta_{\bar t} - \theta.
\end{equation*}
Using this in the first order Saddlepoint approximation (\ref{eq-saddle-3}), we obtain that the Saddlepoint approximation for the average $\bar T = n^{-1}\sum_{i=1}^n T(X_i)$, \note{maybe justify that $T(X)$ is also in exp fam with same cgf and param, which is why the rest works} where $X_1, \ldots, X_n \simiid P_\theta$, is
\begin{align*}
    g_3(\bar t; K_\theta) 
    &= \sqrt{n} (2\pi)^{-p/2}|K_\theta''(\hat\theta_{\bar t} - \theta)|^{-1/2} \expf{nK_\theta(\hat\theta_{\bar t} - \theta) - (\hat\theta_{\bar t} - \theta)^\top \bar t}\\
    &= \sqrt{n} (2\pi)^{-p/2}|\mathcal{H}''(\hat\theta_{\bar t})|^{-1/2} \expf{n(\mathcal{H}(\hat\theta) - \mathcal{H}(\theta_{\bar t})) - (\hat\theta_{\bar t} - \theta)^\top \bar t}\\
    &= \sqrt{n} (2\pi)^{-p/2}|j(\hat\theta_{\bar t})|^{-1/2} \expf{\ell(\theta; \bar t) - \ell(\hat\theta_{\bar t}; \bar t)},
\end{align*}
where we have used that $\mathcal{H}''(\hat\theta)$ is equal to the observed Fisher information $j(\hat\theta)$. Daniels \cite{daniels1958} notes that this approximation can further be used to approximate the distribution of the maximum likelihood estimator. Let $\hat\Theta$ be the random variable solving the score equation $\mathcal{H}'(\hat\Theta) = \bar T$, then, by change of variable, we can use the approximation above to construct an approximation $p^*$ to the distribution of $\hat\Theta$,
\begin{equation*}
    p^*(\hat\theta; \theta, \bar t) = \sqrt{n} (2\pi)^{-p/2}|j(\hat\theta)|^{-1/2} \expf{\ell(\theta; \bar t) - \ell(\hat\theta; \bar t)} \abs{\frac{\d \hat\theta}{\d \bar t}}^{-1}.
\end{equation*}
To compute the determinant of the Jacobian of the transformation $\hat\theta(\bar t)$, we can differentiate the score equation with respect to $\hat\theta$ to find $\mathcal{H}''(\hat\theta) = (\d \bar t / \d \hat\theta)$ and hence $(\d \hat\theta / \d \bar t) = \mathcal{H}''(\hat\theta)^{-1} = j(\hat\theta)^{-1}$, giving
\begin{equation} \label{eq-pstar}
    p^*(\hat\theta; \theta, \bar t) = \sqrt{n} (2\pi)^{-p/2}|j(\hat\theta)|^{1/2} \expf{\ell(\theta; \bar t) - \ell(\hat\theta; \bar t)}.
\end{equation}
While the dependence on $\bar t$ naturally comes from the proposed derivation of the $p^*$ approximation, it is often more convenient to parametrize the loglikelihood and $p^*$ approximations in terms of the maximum likelihood estimator $\hat\theta(\bar t)$. This is possible since we have assumed that the score equations (\ref{eq-score-expfam}) is uniquely solvable which implies that the maximum likelihood estimator $\hat\theta(\bar t)$ is sufficient \note{actually, prob. don't need uniqueness}. We will then write
\begin{equation*}
    p^*(\hat\theta; \theta, \hat\theta) = \sqrt{n} (2\pi)^{-p/2}|j(\hat\theta)|^{1/2} \expf{\ell(\theta; \hat\theta) - \ell(\hat\theta; \hat\theta)}.
\end{equation*}
This also highlights the fact that the $p^*$ approximation inherited its locality from the Saddlepoint approximation, since the density $p^*(\hat\theta; \theta, \hat\theta)$ is different at each point $\hat\theta$ at which it is evaluated. The $p^*$ approximation can also be used in many different situation where the distribution of refence is not necessarily an exponential family. It has been derived and studied in a much broader generality by a series of articles and books by Barndorff-Nielsen \cite{BarndorffNielsen1980,BarndorffNielsen1983}.  

Suppose now that the exponential family $\mathcal{P}$ has an alternative parametrization $\{ P_\phi \}$ such that there exists a diffeomorphism $\phi = \phi(\theta)$ satisfying $\hat\phi = \phi(\hat\theta)$, where $\hat\phi$ and $\hat\theta$ are the maximum likelihood estimators in their respective parametrizations. Then, 
\begin{align*}
    p^*(\hat\phi; \phi, \hat \phi) 
&= \sqrt{n} (2\pi)^{-p/2}|j_\phi(\hat\phi)|^{1/2} \expf{\ell(\phi; \hat \phi) - \ell(\hat\phi; \hat \phi)}\\
&= \sqrt{n} (2\pi)^{-p/2}\left(|j_\theta(\theta(\hat\phi))|\abs{\frac{\d \hat\theta}{\d \hat\phi}}^{-2}\right)^{1/2} \expf{\ell(\theta(\phi); \theta(\hat \phi)) - \ell(\theta(\hat\phi); \theta(\hat \phi))}\\
&= p^*(\theta(\hat\phi); \theta(\phi), \theta(\hat \phi))\abs{\frac{\d \hat\theta}{\d \hat\phi}}^{-1}.
\end{align*}
Hence, the $p^*$ approximation is \textit{invariant under reparametrization}. 

\begin{example}
    Consider now estimating the density of the maximum likelihood estimator of the parameter $\lambda \in \R_+$ of an exponential distribution. The density of the distribution $\text{Exp}(\lambda)$ is
    \begin{equation*}
        f_\lambda(x) = \lambda \expf{-\lambda x}.
    \end{equation*}
    To make direct use of the $p^*$ approximation in Equation (\ref{eq-pstar}), we must work in the natural parametrization of the exponential distribution. For $\lambda \in R_+$, the corresponding natural parameter is $\theta = -\lambda \in \R_-$ and the density of $\text{Exp}(\theta)$ is then $f_\theta(x) = \expf{ \theta x + \logf{-\theta}}$. Given a i.i.d.\,sample $x_1, \ldots, x_n$ of $\text{Exp}(\theta)$, we have the likelihood
    \begin{equation*}
        \ell(\theta; \bar x) = n\left[\theta \bar x + \logf{-\theta}\right],
    \end{equation*}
    where we used that the sufficient statistic is $T(x) = x$ and hence $\bar t = \bar x$ is the sample mean. The maximum likelihood estimator of $\theta$ is then $\hat\theta = -1/\bar x$ and the observed information is equal to $j(\theta) = 1/\theta^2$. The $p^*$ approximation to the density of $\hat\theta$ is then
    \begin{align*}
        p^*(\hat\theta; \theta, \hat\theta) 
        &= \sqrt{n} \frac{|\theta|^n}{|\hat\theta|^{n-1}}\expf{-n(\theta - \hat\theta)/\hat\theta} / \sqrt{2\pi}\\
        &= \sqrt{n} \frac{|\theta|^n}{|\hat\theta|^{n-1}}\expf{n\left[1 - \frac{\theta}{\hat\theta}\right]} / \sqrt{2\pi}
    \end{align*}
    Using the invariance of the $p^*$ approximation, we can obtain a $p^*$ approximation to the density of the maximum likelihood parameter $\hat\lambda$ in the original parametrization,
    \begin{equation*}
        p^*(\hat\lambda; \lambda, \hat\lambda) = p^*(\theta(\hat\lambda); \theta(\lambda), \theta(\hat\lambda)) \abs{\d \hat\theta / \d \hat\lambda}^{-1} = \sqrt{n} \frac{|\lambda|^n}{|\hat\lambda|^{n-1}}\expf{n\left[\frac{\lambda}{\hat\lambda} - 1\right]} / \sqrt{2\pi}.
    \end{equation*}
    Since $\text{Exp}(\lambda) = \Gamma(1, \lambda)$, the distribution of $\bar X$ is $\Gamma(n, n\lambda)$ and hence $\hat\lambda = \bar x$ is $\text{Inv-}\Gamma(n, n\lambda)$. We can compare the $p^*$ approximation to the commonly used Normal approximation to the distribution of the maximum likelihood estimator. In the exponential model, the Fisher information is $I(\lambda) = \lambda^{-2}$ and the following central limit theorem holds for the maximum likelihood estimator
    \begin{equation*}
        \sqrt{n}(\hat\lambda - \lambda) \xrightarrow[]{\d} N(0, I(\lambda)^{-1})\ \ \ \  \text{as } n \rightarrow \infty,
    \end{equation*}
    hence, $\hat\lambda$ is approximately $N(\lambda, \lambda^2/n)$ with an approximation error of the density of $o(n^{-1/2})$. In Figure \ref{fig-pstar-approx}, we compare these two approximations to the density of the MLE $\hat\lambda$ for $\lambda = 2$. As we can see in the left panel, the $p^*$ approximation properly fits the true density of $\hat\lambda$ and captures the bias of the $\hat\lambda$ estimator as opposed to the Normal approximation which is centered around the true value of $\lambda$. Furthermore, we can observe in the right panel how the relative error of the $p^*$ approximation is identical to the approximation error of the Saddlepoint approximation to the mean of $\Gamma(2, 1)$ seen in Example \ref{ex-gamma-saddle}. This is also a direct consequence of the invariance of the $p^*$ approximation since $\hat\Lambda$, the random variable associated to the MLE, is the inverse of the sample mean $\bar X$, which is a diffeomorphic transformation for positive reals.
    
    \begin{figure}
        \textbf{Error of $p^*$ approximation of MLE in $\textrm{Exp}(\lambda)$ model with $n=10$}
        \centering
        \subfloat{
            \includegraphics[width=8cm]{pstar_exp_dens.eps} 
        }
        \subfloat{
            \includegraphics[width=8cm]{pstar_exp_err.eps} 
        }
        \caption{Study of the approximation error of the Saddlepoint approximation on a standardized sum of $n=10$ of $\Gamma(2, 1)$ random variables. Both panel exposes properties studied of the Saddlepoint approximation: the accurate rellative error, the gain in order of approximation and the uniform relative error of the approximation for sums of Gamma random variables.}
        \label{fig-pstar-approx}
    \end{figure}    

    \input{thm-char-integrable-convolution}

\end{example}
