
We now turn ourselves to applying the approximations developed in Section \note{todo} on the problem of testing two nested Gaussian graphical models. More precisely, let $\G = (\Gamma, E)$ and $\G_0 = (\Gamma, E_0)$ with $E_0 \subset E$, we are interested in developing a test for the problem
\begin{equation} 
    H_0 : \Omega \in \S_{\succ 0}(\G_0)\ \ \t{ vs. }\ \ H_1 : \Omega \in \S_{\succ 0}(\G) \setminus \S_{\succ 0}(\G_0).
\end{equation}




\begin{lemma}
    \cite{eriksen1996tests} Let $\G$ be a graph, $\Omega \in \S_{\succ 0}(\G)$ and $S$ be a sample covariance matrix computes from a sample of size $n$ of the Gaussian graphical model associated to $\G$ with precision matrix $\Omega$. Then, the density $p(S; \Omega)$ of $S$ satisfies
    \begin{equation*}
        p(S; \Omega) = c \frac{|\Omega|^{n/2}}{|\hat\Omega_\G|^{n/2}} |j_\G(\hat\Omega_\G) |^{-1/2} \expfc{-\frac{n}{2} \trB{\Omega S}} (1 + O(n^{-3/2})),
    \end{equation*}
    where $c$ is a normalization constant and $j_G$ is the observed information matrix given by
    \begin{equation} \label{eq-obs-info}
        j_\G(\Omega)_{a b} = \trB{\Omega^{-1} H_a \Omega^{-1} H_b} \t{ for all } a, b  \in E.
    \end{equation}
\end{lemma}

\begin{proof}
    Since $XX^\top$ is sufficient in the Gaussian graphical model, $S$ corresponds to the sample average of the sufficient statistic. Applying the Saddlepoint approximation to $S$ as done in Section \note{todo}, we have that the density of $S$ satisfies
    \begin{equation*}
        p(S; \Omega) = c |j_\G(\hat\Omega_\G)|^{-1/2} \expfc{\ell(\Omega; S) - \ell(\hat\Omega_\G; S)} (1 + O(n^{-3/2})).
    \end{equation*}
    Replacing the log-likelihood of the Gaussian graphical model in the above equation, we get
    \begin{align*}
        p(S; \Omega) 
        &\propto |j_\G(\hat\Omega_\G)|^{-1/2} \expfc{\ell(\Omega; S) - \ell(\hat\Omega_\G; S)} (1 + O(n^{-3/2})) \\
        &\propto |j_\G(\hat\Omega_\G)|^{-1/2} 
        \frac{|\Omega|^{n/2}}{|\hat\Omega_\G|^{n/2}}
        \expfc{-\frac{n}{2}\left(\trB{\Omega S} - \trB{\hat\Omega_\G S}\right)} (1 + O(n^{-3/2}))\\
        &\propto |j_\G(\hat\Omega_\G)|^{-1/2} 
        \frac{|\Omega|^{n/2}}{|\hat\Omega_\G|^{n/2}}
        \expfc{-\frac{n}{2}\trB{\Omega S}} (1 + O(n^{-3/2}))
    \end{align*}
    where we use that $\hat\Omega_\G S = 1_p$ and hence $\trB{\hat\Omega_\G S} = p$ is constant. The formula for the observed infomation matrix can be easily verified by taking the appropriate derivatives of the log-likelihood.
\end{proof}

% \begin{align*}
%     p(S_e | S_{\G_0}; \Omega_0) = \frac{p(S_e, S_{\G_0}; \Omega_0)}{p(S_{\G_0}; \Omega_0)}
% \end{align*}

We now focus our attention to computing the determinant of the observed information matrix given in (\ref{eq-obs-info}). Let $a, b \in E$, we start by noting that for any $S \in \S(\G)$ and $i, j \in [p]$
\begin{equation*}
    (SH_a)_{ij} = \begin{cases}
        & S_{i \bar j}\ \ \t{ if } j \in a\\
        &0\ \ \ \ \  \t{otherwise}
    \end{cases}
    \ \ \ \t{ and }\ \ \ \ \ 
    (SH_b)_{ji} = \begin{cases}
        & S_{j \bar i}\ \ \t{ if } i \in b\\
        &0\ \ \ \ \  \t{otherwise}
    \end{cases},
\end{equation*}
where $\bar j \in a$ and $\bar i \in b$ are such that $\eset{j} \cup \eset{\bar j} = a$ and $\eset{i} \cup \eset{\bar i} = b$. Using this in the expression of $j_\G(S)_{ab}$, we get
\begin{equation} \label{eq-tr-sum}
    \trB{SH_aSH_b} = \sum_{i=1, j=1}^p (SH_a)_{ij}(SH_b)_{ji} = \sum_{(i, j) \in a \times b} S_{i \bar j} S_{j \bar i} = \sum_{e \in a \times b} S_e S_{\bar e},
\end{equation}
where the last equality only involves re-indexing and re-ordering the sum. We now inspect the different $a, b$ for which the summand $S_eS_{\bar e}$ is non-zero. Since we chose $S = 1_p + s H_{e_0}$, we have that
\begin{equation*}
    S_e = \begin{cases}
        &1\ \ \t{ if } e = \eset{ i } \t{ for } i \in [p]\\
        &s\ \ \t{ if } e = e_0\\
        &0\ \ \t{ otherwise}.
    \end{cases}
\end{equation*}
Hence the summands $S_e S_{\bar e}$ of (\ref{eq-tr-sum}) are non-zero if and only if both $e$ and $\bar e$ are either $\eset{i}$ for some $i \in [p]$ or equal to $e_0$. Without loss of generality, we will assume that $e_0 = \eset{1, 2}$. This leaves us with the following cases. 

Case 1. $e = \bar e = \eset{1, 2}$. This can only happen if $a = \eset{1}$ and $b = \eset{2}$, in which case
\begin{equation*}
    j_\G(S)_{\eset{1} \eset{2}} = \sum_{e \in \eset{1} \times \eset{2}} S_e S_{\bar e} = S_{\eset{1, 2}}S_{\eset{1, 2}} = s^2.
\end{equation*}

Case 2. $e = \eset{1, 2}$ and $\bar e = \eset{i, j}$ with $i \neq j$ (or the opposite). Then we must have $a = \eset{1, i}$ and $b = \eset{2, j}$ and
\begin{equation*}
    j_\G(S)_{\eset{1, i} \eset{2, j}} = S_{1 2}S_{i j} + S_{1 j} S_{i 2} + S_{i 2} S_{1 j} + S_{i j} S_{1 2} = 0,
\end{equation*}
since we assumed that $S = 1_p + s H_{e_0}$.

Case 3. $e = \eset{1, 2}$ and $\bar e = \eset{i}$ (or the opposite). This is the case when $a = \eset{1, i}$ and $b = \eset{2, i}$ and we get
\begin{equation*}
    j_\G(S)_{\eset{1, i} \eset{2, i}} = 
    S_{1 2}S_{i i} + S_{1 i} S_{i 2} + S_{i 2} S_{1 i} + S_{i i} S_{1 2} = 2 s,
\end{equation*}
since $S_{i i} = 1$, $S_{1, 2} = s$ and $S_{i 1} = S_{i 2} = 0$. Note that since the matrix $j_\G$ is indexed by edges in $\G$, the cases $a = \eset{1, i}$ and $b = \eset{2, i}$ are only relevant if $a, b \in E$ and so this case only for $i \in C = \t{bd}(1) \cap \t{bd}(2)$.

Case 4. $e = \eset{i}$ and $\bar e = \eset{j}$. This only happens is $a = b = \eset{i, j}$, which, again using that $a, b \in E$ and $S = 1_p + sH_{e_0}$ gives
\begin{equation*}
    j_\G(S)_{\eset{i, j} \eset{i, j}} = \begin{cases}
        &S_\eset{i, i}S_\eset{i, i} = 1\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \t{ if } i = j\\
        &2S_\eset{1 1}S_\eset{2 2} + 2S_\eset{1 2}S_\eset{1 2} = 2 + 2s^2\  \t{ if } \eset{i, j} = \eset{1, 2}\\
        &2S_\eset{i,i} + 2S_\eset{i, j} = 2\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \t{ otherwise.}
    \end{cases}
\end{equation*}
This shows that $j_\G(S)$ is a block diagonal matrix with blocks each equal to one of the following matrices
\begin{equation*}
    A = \bordermatrix{~ & \eset{1} & \eset{2} & \eset{1, 2} \cr
        \eset{1} & 1 & s^2 & 2s \cr
        \eset{2} & s^2 & 1 & 2s \cr
        \eset{1, 2} & 2s & 2s & 2 + 2s^2 \cr
    }
    \ \ \ \ \ \ \ \ \ 
    B_i = \bordermatrix{~ & \eset{1, i} & \eset{2, i} \cr
        \eset{1, i} & 2 & 2s \cr
        \eset{2, i} & 2s & 2 \cr
    }, i \in C.
\end{equation*}
Since $|A| = 2 (1-s)^3$ and $|B_i| = 4(1-s)$ for $i \in C$, we have that the determinant of the observed information $\abs{j_\G(1_p + sH_{e_0})} \propto (1-s)^{3 + |C|}$.