\subsection{Hypothesis testing}


We now turn ourselves to applying the approximations developed in Section \note{todo} on the problem of testing two nested Gaussian graphical models. More precisely, let $\G = (\Gamma, E)$ and $\G_0 = (\Gamma, E_0)$ with $E_0 \subset E$, we are interested in developing a test for the problem
\begin{equation} 
    H_0 : \Omega \in \S_{\succ 0}(\G_0)\ \ \t{ vs. }\ \ H_1 : \Omega \in \S_{\succ 0}(\G) \setminus \S_{\succ 0}(\G_0).
\end{equation}




\begin{lemma} \label{lem-pstar-covariance}
    \cite{eriksen1996tests} Let $\G$ be a graph, $\Omega \in \S_{\succ 0}(\G)$ and $S$ be a sample covariance matrix computes from a sample of size $n$ of the Gaussian graphical model associated to $\G$ with precision matrix $\Omega$. Then, the density $p(S; \Omega)$ of $S$ satisfies
    \begin{equation*}
        p(S; \Omega) = c \frac{|\Omega|^{n/2}}{|\hat\Omega_\G(S)|^{n/2}} |j_\G(\hat\Omega_\G(S)) |^{-1/2} \expfc{-\frac{n}{2} \trB{\Omega S}} (1 + O(n^{-3/2})),
    \end{equation*}
    where $c$ is a normalization constant, $\hat\Omega_\G(S)$ is the maximum likelihood of $\Omega$ assuming the graph $\G$ and given the data $S$, and $j_G$ is the observed information matrix given by
    \begin{equation} \label{eq-obs-info}
        j_\G(\Omega)_{a b} = \trB{\Omega^{-1} H_a \Omega^{-1} H_b} \t{ for all } a, b  \in E.
    \end{equation}
\end{lemma}

\begin{proof}
    Since $XX^\top$ is sufficient in the Gaussian graphical model, $S$ corresponds to the sample average of the sufficient statistic. Applying the Saddlepoint approximation to $S$ as done in Section \note{todo}, we have that the density of $S$ satisfies
    \begin{equation*}
        p(S; \Omega) = c |j_\G(\hat\Omega_\G(S))|^{-1/2} \expfc{\ell(\Omega; S) - \ell(\hat\Omega_\G(S); S)} (1 + O(n^{-3/2})).
    \end{equation*}
    Replacing the log-likelihood of the Gaussian graphical model in the above equation, we get
    \begin{align*}
        p(S; \Omega) 
        &\propto |j_\G(\hat\Omega_\G(S))|^{-1/2} \expfc{\ell(\Omega; S) - \ell(\hat\Omega_\G(S); S)} (1 + O(n^{-3/2})) \\
        &\propto |j_\G(\hat\Omega_\G(S))|^{-1/2} 
        \frac{|\Omega|^{n/2}}{|\hat\Omega_\G(S)|^{n/2}}
        \expfc{-\frac{n}{2}\left(\trB{\Omega S} - \trB{\hat\Omega_\G(S) S}\right)} (1 + O(n^{-3/2}))\\
        &\propto |j_\G(\hat\Omega_\G(S))|^{-1/2} 
        \frac{|\Omega|^{n/2}}{|\hat\Omega_\G(S)|^{n/2}}
        \expfc{-\frac{n}{2}\trB{\Omega S}} (1 + O(n^{-3/2}))
    \end{align*}
    where we use that $\hat\Omega_\G(S) S = 1_p$ and hence $\trB{\hat\Omega_\G(S) S} = p$ is constant. The formula for the observed infomation matrix can be easily verified by taking the appropriate derivatives of the log-likelihood.
\end{proof}

Consider now a Gaussian graphical model with graph $\G = ([p], E)$ and the sub-model constructed by remove an edge $a \in E$ from $\G$. The sub-model is then the Gaussian graphical model associated to the graph $\G_0 = ([p], E_0)$ where $E_0 = E \setminus \eset{e_0}$. Without loss of generality, we assume $e_0 = \eset{1,2}$.
Let $S$ be the sample covariance matrix and assume that the true parameter is $\Omega_0 \in \S_{\succ 0}(\G_0)$. We can then construct approximations to the densities $p(S^{\G_0}; \Omega_0)$ and $p(S^{\G}; \Omega_0)$ by applying Lemma \ref{lem-pstar-covariance} in the Gaussian graphical models associated to $\G$ and $\G_0$ to get
\begin{equation*}
    p(S^{\G_0}; \Omega_0) = c \frac{|\Omega_0|^{n/2}}{|\hat\Omega_{\G_0}(S^{\G_0})|^{n/2}} |j_{\G_0}(\hat\Omega_{\G_0}(S^{\G_0})) |^{-1/2} \expfc{-\frac{n}{2} \trB{\Omega_0 S^{\G_0}}} (1 + O(n^{-3/2}))
\end{equation*}
and
\begin{equation*}
    p(S^\G; \Omega_0) = c \frac{|\Omega_0|^{n/2}}{|\hat\Omega_\G(S^\G)|^{n/2}} |j_\G(\hat\Omega_\G(S^\G)) |^{-1/2} \expfc{-\frac{n}{2} \trB{\Omega_0 S^\G}} (1 + O(n^{-3/2})).
\end{equation*}
Combining these results together allows us to approximate the density of $S_e$ conditioned on $S^{\G_0}$
\begin{align*}
    p(S_{e_0}| S^{\G_0}; \Omega_0)
    &= \frac{p(S_{e_0}, S^{\G_0}; \Omega_0)}{p(S^{\G_0}; \Omega_0)} = \frac{p(S^{\G}; \Omega_0)}{p(S^{\G_0}; \Omega_0)}\\
    & \doteq \tilde c 
    \frac
        {|\Omega_0|^{n/2}|\hat\Omega_{\G_0}(S^{\G_0})|^{-n/2} |j_{\G_0}(\hat\Omega_{\G_0}(S^{\G_0})) |^{-1/2} \expfc{-\frac{n}{2} \trB{\Omega_0 S^{\G_0}}}}
        {|\Omega_0|^{n/2}|\hat\Omega_\G(S^\G)|^{-n/2} |j_\G(\hat\Omega_\G(S^\G)) |^{-1/2} \expfc{-\frac{n}{2} \trB{\Omega_0 S^\G}}}\\
    &= \tilde c 
    \frac
        {|\hat\Omega_\G(S^\G)|^{n/2} |j_\G(\hat\Omega_\G(S^\G)) |^{1/2}}    
        {|\hat\Omega_{\G_0}(S^{\G_0})|^{n/2} |j_{\G_0}(\hat\Omega_{\G_0}(S^{\G_0})) |^{1/2}}\\
    &= q^{n/2} \frac{|j_\G(\hat\Omega_\G(S^\G)) |^{1/2}}    {|j_{\G_0}(\hat\Omega_{\G_0}(S^{\G_0})) |^{1/2}}.
\end{align*}
where we used in the last step that $\Omega_0 S^\G = \Omega_0 S^{\G_0}$ which leads to the exponential terms canceling out. Since $w \xrightarrow{d} \chi^2_1$ is asymptotically ancillary, and $q = \expfc{-\frac{1}{2} w}$, $q$ is also asymptotically ancillary. Since $S^{\G_0}$ is a complete sufficient statistic assuming $\G_0$, $S^{\G_0}$ and $q$ are asymptotically independent by Basu's Theorem \cite{10.2307/25048259}. This means that asymptotically $p(S_{e_0}; \Omega_0) = p(S_{e_0} | S^{\G_0}; \Omega_0)$ for any $S^{\G_0}$ and we can chose $S^{\G_0}$ freely in the previous equation. Taking $S^{\G_0} = 1_p$ gives
\begin{equation*}
    S^\G = 1_p + S_{e_0} H_{e_0} = \begin{pmatrix}
        1   & S_{e_0} & \\
        S_{e_0} & 1 & \\
            &   & 1_{p-2}
    \end{pmatrix}.
\end{equation*}
Since $S^{\G_0}$ and $S^\G$ are positive definite, they are their own positive definite completion and we have $|\hat\Omega_\G(S^\G)| = |S^\G|^{-1} = (1 - S_{e_0}^2)^{-1}$ and $|\hat\Omega_{\G_0}(S^{\G_0})| = |S^{\G_0}|^{-1} = 1$, giving $q = 1 - S_{e_0}^2$. A change of variable from $S_e$ to $q$ has Jacobian $(1 - q)^{-1/2}$ and the density of $q$ can be given by
\begin{equation*}
    p(q) \doteq \hat c q^{n/2} |j_{\G}(\hat\Omega_{\G}(S^{\G})) |^{-1/2} (1 - q)^{-1/2},
\end{equation*}
for some normalizing constant $\hat c$.

We now focus our attention to computing the determinant of the observed information matrix given in (\ref{eq-obs-info}) with $\Omega^{-1} = 1_p + S_e H_e$. Let $a, b \in E$, we start by noting that for any $S \in \S(\G)$ and $i, j \in [p]$
\begin{equation*}
    (SH_a)_{ij} = \begin{cases}
        & S_{i \bar j}\ \ \t{ if } j \in a\\
        &0\ \ \ \ \  \t{otherwise}
    \end{cases}
    \ \ \ \t{ and }\ \ \ \ \ 
    (SH_b)_{ji} = \begin{cases}
        & S_{j \bar i}\ \ \t{ if } i \in b\\
        &0\ \ \ \ \  \t{otherwise}
    \end{cases},
\end{equation*}
where $\bar j \in a$ and $\bar i \in b$ are such that $\eset{j} \cup \eset{\bar j} = a$ and $\eset{i} \cup \eset{\bar i} = b$. Using this in the expression of $j_\G(S)_{ab}$, we get
\begin{equation} \label{eq-tr-sum}
    \trB{SH_aSH_b} = \sum_{i=1, j=1}^p (SH_a)_{ij}(SH_b)_{ji} = \sum_{(i, j) \in a \times b} S_{i \bar j} S_{j \bar i} = \sum_{e \in a \times b} S_e S_{\bar e},
\end{equation}
where the last equality only involves re-indexing and re-ordering the sum. We now inspect the different $a, b$ for which the summand $S_eS_{\bar e}$ is non-zero. Since we chose $S = 1_p + s H_{e_0}$, we have that
\begin{equation*}
    S_e = \begin{cases}
        &1\ \ \t{ if } e = \eset{ i } \t{ for } i \in [p]\\
        &s\ \ \t{ if } e = e_0\\
        &0\ \ \t{ otherwise}.
    \end{cases}
\end{equation*}
Hence the summands $S_e S_{\bar e}$ of (\ref{eq-tr-sum}) are non-zero if and only if both $e$ and $\bar e$ are either $\eset{i}$ for some $i \in [p]$ or equal to $e_0$. Without loss of generality, we will assume that $e_0 = \eset{1, 2}$. This leaves us with the following cases. 

Case 1. $e = \bar e = \eset{1, 2}$. This can only happen if $a = \eset{1}$ and $b = \eset{2}$, in which case
\begin{equation*}
    j_\G(S)_{\eset{1} \eset{2}} = \sum_{e \in \eset{1} \times \eset{2}} S_e S_{\bar e} = S_{\eset{1, 2}}S_{\eset{1, 2}} = s^2.
\end{equation*}

Case 2. $e = \eset{1, 2}$ and $\bar e = \eset{i, j}$ with $i \neq j$ (or the opposite). Then we must have $a = \eset{1, i}$ and $b = \eset{2, j}$ and
\begin{equation*}
    j_\G(S)_{\eset{1, i} \eset{2, j}} = S_{1 2}S_{i j} + S_{1 j} S_{i 2} + S_{i 2} S_{1 j} + S_{i j} S_{1 2} = 0,
\end{equation*}
since we assumed that $S = 1_p + s H_{e_0}$.

Case 3. $e = \eset{1, 2}$ and $\bar e = \eset{i}$ (or the opposite). This is the case when $a = \eset{1, i}$ and $b = \eset{2, i}$ and we get
\begin{equation*}
    j_\G(S)_{\eset{1, i} \eset{2, i}} = 
    S_{1 2}S_{i i} + S_{1 i} S_{i 2} + S_{i 2} S_{1 i} + S_{i i} S_{1 2} = 2 s,
\end{equation*}
since $S_{i i} = 1$, $S_{1, 2} = s$ and $S_{i 1} = S_{i 2} = 0$. Note that since the matrix $j_\G$ is indexed by edges in $\G$, the cases $a = \eset{1, i}$ and $b = \eset{2, i}$ are only relevant if $a, b \in E$ and so this case only for $i \in C = \t{bd}(1) \cap \t{bd}(2)$.

Case 4. $e = \eset{i}$ and $\bar e = \eset{j}$. This only happens is $a = b = \eset{i, j}$, which, again using that $a, b \in E$ and $S = 1_p + sH_{e_0}$ gives
\begin{equation*}
    j_\G(S)_{\eset{i, j} \eset{i, j}} = \begin{cases}
        &S_\eset{i, i}S_\eset{i, i} = 1\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \t{ if } i = j\\
        &2S_\eset{1 1}S_\eset{2 2} + 2S_\eset{1 2}S_\eset{1 2} = 2 + 2s^2\  \t{ if } \eset{i, j} = \eset{1, 2}\\
        &2S_\eset{i,i} + 2S_\eset{i, j} = 2\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \t{ otherwise.}
    \end{cases}
\end{equation*}
This shows that $j_\G(S)$ is a block diagonal matrix with blocks each equal to one of the following matrices
\begin{equation*}
    A = \bordermatrix{~ & \eset{1} & \eset{2} & \eset{1, 2} \cr
        \eset{1} & 1 & s^2 & 2s \cr
        \eset{2} & s^2 & 1 & 2s \cr
        \eset{1, 2} & 2s & 2s & 2 + 2s^2 \cr
    }
    \ \ \ \ \ \ \ \ \ 
    B_i = \bordermatrix{~ & \eset{1, i} & \eset{2, i} \cr
        \eset{1, i} & 2 & 2s \cr
        \eset{2, i} & 2s & 2 \cr
    }, i \in C.
\end{equation*}
Since $|A| = 2 (1-s)^3$ and $|B_i| = 4(1-s)$ for $i \in C$, we have that the determinant of the observed information $\abs{j_\G(1_p + sH_{e_0})} \propto (1-s)^{3 + |C|}$.